# LinkLater - Archive Intelligence & Binary Extraction

**Complete text extraction from web archives (Common Crawl + Wayback Machine) with automatic binary file support, entity extraction, and link graph building.**

---

## üöÄ What LinkLater Does

LinkLater is a **comprehensive archive intelligence system** that:

‚úÖ **Extracts text from binary files** (PDF, DOCX, XLSX, PPTX) found in archives
‚úÖ **Three-tier fallback chain**: Common Crawl ‚Üí Wayback Machine ‚Üí Firecrawl
‚úÖ **Extracts entities**: Companies, Persons, Registration Numbers
‚úÖ **Builds link graphs**: Outlinks, backlinks, domain relationships
‚úÖ **One-command pipelines**: Automated end-to-end workflows
‚úÖ **Battle-tested**: Production-ready, comprehensive error handling

---

## ‚ö° Quick Start (One-Command Pipelines)

### Extract All PDFs from a Domain

```bash
cd /Users/attic/DRILL_SEARCH/drill-search-app/python-backend/modules/linklater/pipelines

# Single command - finds PDFs, extracts text, saves results
./extract_domain_pdfs.sh "company.com"
```

**What it does:**
1. Searches Common Crawl index for all PDFs on domain
2. Extracts text from each PDF (CC ‚Üí Wayback ‚Üí Firecrawl fallback)
3. Saves extracted content to `company.com_pdfs.json`

### Extract All Documents (PDF + DOCX + XLSX + PPTX)

```bash
./extract_domain_docs.sh "example.com"
```

**What it does:**
1. Finds all document types (.pdf, .docx, .xlsx, .pptx) on domain
2. Extracts text from all documents
3. Extracts entities (companies, persons, registrations)
4. Saves to `linklater_results/example.com_documents.json`

### Full Entity Extraction + Knowledge Graph

```bash
./full_entity_extraction.sh "tesla.com" tesla_graph.json
```

**What it does:**
1. Discovers all content on domain
2. Extracts all binary documents
3. Extracts entities from each page/document
4. **Extracts outlinks** (links from domain to external sites)
5. **Extracts backlinks** (links to domain from other sites)
6. Builds knowledge graph with nodes (pages, companies, persons) and edges (mentions, links_to)
7. Saves graph to `tesla_graph.json`

---

## üîß CLI Usage (Direct)

### Single URL Extraction

```bash
# Extract PDF text
python linklater.py --url "https://example.com/annual-report.pdf"

# Extract Word document
python linklater.py --url "https://example.com/contract.docx" --format markdown

# Extract with verbose output
python linklater.py --url "https://example.com/data.xlsx" --verbose --stats
```

### Batch URL Processing

```bash
# Process multiple URLs from file
python linklater.py --file urls.txt --output results.json

# High concurrency batch
python linklater.py --file urls.txt --concurrent 100 --output batch_results.json
```

### Extract Outlinks and Entities

```bash
# Extract content + outlinks + entities
python -c "
import asyncio
import sys
sys.path.insert(0, '/Users/attic/DRILL_SEARCH/drill-search-app/python-backend')

from modules.linklater.enrichment.cc_enricher import CCEnricher

async def extract():
    enricher = CCEnricher(
        extract_entities=True,
        extract_outlinks=True
    )

    result = await enricher.enrich_single(
        url='https://example.com',
        title='',
        snippet=''
    )

    print(f'Companies: {len(result.companies)}')
    print(f'Persons: {len(result.persons)}')
    print(f'Outlinks: {len(result.outlinks)}')

    for company in result.companies:
        print(f'  - {company}')

    for outlink in result.outlinks[:10]:
        print(f'  ‚Üí {outlink}')

asyncio.run(extract())
"
```

---

## üì¶ Binary File Support

| File Type | Extension | Extraction | Notes |
|-----------|-----------|------------|-------|
| **PDF** | .pdf | ‚úÖ Full | pypdf + pdfplumber |
| **Word (modern)** | .docx | ‚úÖ Full | python-docx |
| **Excel (modern)** | .xlsx | ‚úÖ Full | openpyxl |
| **PowerPoint (modern)** | .pptx | ‚úÖ Full | python-pptx |
| **Word (legacy)** | .doc | ‚ö†Ô∏è Partial | Requires antiword |
| **Excel (legacy)** | .xls | ‚ö†Ô∏è Not yet | Would need xlrd |
| **Archives** | .zip, .tar, .gz | ‚úÖ Lists contents | Built-in |

---

## üîó Link Intelligence Features

### Outlinks (Links FROM Domain)

```python
from modules.linklater.enrichment.cc_enricher import CCEnricher

enricher = CCEnricher(extract_outlinks=True)
result = await enricher.enrich_single('https://company.com')

# result.outlinks contains all external links
for link in result.outlinks:
    print(f"company.com ‚Üí {link}")
```

**What it extracts:**
- All `<a href="...">` links from HTML
- Filters out same-domain links (internal navigation)
- Filters out social media links (Facebook, Twitter, etc.)
- Deduplicates and limits to top 50 outlinks per page

### Backlinks (Links TO Domain)

**Via Common Crawl WebGraph:**

```bash
# Download CC webgraph data
cd ~/cc_webgraph_data

# Host-level graph (domains linking to domains)
curl -L "https://data.commoncrawl.org/projects/hyperlinkgraph/cc-main-2025-sep-oct-nov/host/cc-main-2025-sep-oct-nov-host-edges.txt.gz" -o host-edges.txt.gz

# Page-level graph (URLs linking to URLs)
curl -L "https://data.commoncrawl.org/projects/hyperlinkgraph/cc-main-2025-sep-oct-nov/domain/cc-main-2025-sep-oct-nov-domain-edges.txt.gz" -o domain-edges.txt.gz

# Search for backlinks to your domain
zgrep "example.com" host-edges.txt.gz
```

**Output format:**
```
source_domain  target_domain  link_count
competitor.com example.com    15
news-site.org  example.com    3
```

### Knowledge Graph Building

The `full_entity_extraction.sh` pipeline automatically builds a graph with:

**Nodes:**
- `page` - Web pages/documents
- `company` - Extracted companies
- `person` - Extracted persons
- `external` - External URLs (outlinks)

**Edges:**
- `mentions` - Page mentions entity
- `links_to` - Page links to external URL

```json
{
  "nodes": [
    {"id": "page_0", "type": "page", "url": "https://example.com/about"},
    {"id": "company_1", "type": "company", "name": "Acme Corp"},
    {"id": "person_2", "type": "person", "name": "John Smith"},
    {"id": "external_3", "type": "external", "url": "https://partner.com"}
  ],
  "edges": [
    {"source": "page_0", "target": "company_1", "type": "mentions"},
    {"source": "page_0", "target": "person_2", "type": "mentions"},
    {"source": "page_0", "target": "external_3", "type": "links_to"}
  ]
}
```

---

## üîÑ Fallback Chain (Automatic)

LinkLater tries sources in order until content is found:

```
1. Common Crawl (free, bulk archives)
   ‚îú‚îÄ Checks CC CDX index
   ‚îú‚îÄ Fetches WARC record
   ‚îî‚îÄ Extracts binary if PDF/DOCX/etc
         ‚Üì NOT FOUND

2. Wayback Machine (free, comprehensive historical)
   ‚îú‚îÄ Queries Wayback availability API
   ‚îú‚îÄ Fetches snapshot with id_ flag (no toolbar)
   ‚îî‚îÄ Extracts binary if needed
         ‚Üì NOT FOUND

3. Firecrawl (paid, live scraping)
   ‚îú‚îÄ Requires FIRECRAWL_API_KEY env var
   ‚îú‚îÄ Scrapes live web
   ‚îî‚îÄ Returns markdown content
         ‚Üì NOT FOUND

4. FAILED
   ‚îî‚îÄ Returns error
```

**No configuration needed** - fallback is automatic. If FIRECRAWL_API_KEY is set, it will be used as last resort.

---

## üìä Output Formats

### JSON Output

```json
{
  "url": "https://example.com/report.pdf",
  "source": "wayback",
  "content": "Extracted text from PDF...",
  "status": 200,
  "latency_ms": 350,
  "timestamp": "20240101123456",
  "error": null
}
```

### Markdown Output

```bash
python linklater.py --url "https://example.com" --format markdown
```

Returns clean markdown text ready for processing.

### Entity Extraction Output

```json
{
  "url": "https://example.com/about",
  "companies": [
    {"text": "Acme Corporation", "confidence": 0.95},
    {"text": "Example Holdings Ltd", "confidence": 0.88}
  ],
  "persons": [
    {"text": "John Smith", "confidence": 0.92},
    {"text": "Jane Doe", "confidence": 0.85}
  ],
  "registrations": [
    {"text": "12345678", "confidence": 0.90}
  ],
  "outlinks": [
    "https://partner1.com/page",
    "https://supplier.org/info"
  ]
}
```

---

## ‚öôÔ∏è Configuration

### Environment Variables

```bash
# Required (for Firecrawl fallback only)
FIRECRAWL_API_KEY=sk-...

# Optional
LINKLATER_CACHE_DIR=/path/to/cache  # Default: /tmp/linklater_cache
LINKLATER_TIMEOUT=15.0              # Request timeout in seconds
```

### CLI Flags

```bash
python linklater.py --help

# Key flags:
--url URL                    # Single URL to scrape
--file FILE                  # File with URLs (one per line)
--extract-binary            # Extract text from PDF/DOCX/etc [default: True]
--no-binary                 # Disable binary extraction (HTML only)
--cc-only                   # Common Crawl only (no Wayback/Firecrawl)
--format {json,markdown,text} # Output format
--concurrent N              # Max concurrent requests
--verbose                   # Verbose output with progress
--stats                     # Print statistics
--output FILE               # Output file (default: stdout)
```

---

## üß™ Testing

### Test Binary Extraction

```bash
cd /Users/attic/DRILL_SEARCH/drill-search-app/python-backend

# Run test suite
python modules/linklater/tests/test_binary_extraction.py
```

### Test Wayback Fallback

```bash
# Create test script
cat > /tmp/test_wayback.py << 'EOF'
import asyncio
import sys
sys.path.insert(0, '/Users/attic/DRILL_SEARCH/drill-search-app/python-backend')

from modules.linklater.scraping.cc_first_scraper import CCFirstScraper

async def test():
    scraper = CCFirstScraper(extract_binary=True, cc_only=False)
    result = await scraper.get_content("https://www.python.org/")

    print(f"Source: {result.source}")
    print(f"Status: {result.status}")
    print(f"Content length: {len(result.content)} chars")

asyncio.run(test())
EOF

python3 /tmp/test_wayback.py
```

### Test Entity Extraction

```bash
python3 -c "
import asyncio
import sys
sys.path.insert(0, '/Users/attic/DRILL_SEARCH/drill-search-app/python-backend')

from modules.linklater.enrichment.cc_enricher import CCEnricher

async def test():
    enricher = CCEnricher(extract_entities=True, extract_outlinks=True)
    result = await enricher.enrich_single('https://example.com', '', '')

    print(f'Entities: {len(result.companies)} companies, {len(result.persons)} persons')
    print(f'Outlinks: {len(result.outlinks)}')

asyncio.run(test())
"
```

---

## üìà Performance

| Operation | Latency | Notes |
|-----------|---------|-------|
| **CC index lookup** | ~50-100ms | CDX API query |
| **CC WARC fetch** | ~200-300ms | Fetching from S3 |
| **Wayback lookup** | ~150-250ms | Availability API |
| **Wayback fetch** | ~300-500ms | Snapshot retrieval |
| **Firecrawl scrape** | ~2-5s | Live web scraping |
| **PDF extraction** | +150ms | pypdf/pdfplumber |
| **DOCX extraction** | +100ms | python-docx |
| **XLSX extraction** | +200ms | openpyxl (more data) |
| **Entity extraction** | +50-100ms | Regex patterns |
| **Outlink extraction** | +10-20ms | Simple HTML parsing |

**Cache benefits:** LRU cache means repeated URLs = 0ms.

---

## üéØ Use Cases

### 1. FileType Search (PDF Intelligence)

```bash
# Find all PDFs mentioning "annual report" on a domain
./pipelines/extract_domain_pdfs.sh "company.com"

# Then search extracted text
jq '.[] | select(.content | contains("annual report"))' company.com_pdfs.json
```

### 2. InDOM Search (Domain Intelligence)

```bash
# Extract all documents + entities from discovered domains
./pipelines/extract_domain_docs.sh "newly-discovered-domain.com"

# View all companies mentioned
jq '.companies[].text' linklater_results/newly-discovered-domain.com_entities.json
```

### 3. Link Graph Analysis

```bash
# Build complete knowledge graph
./pipelines/full_entity_extraction.sh "target.com" target_graph.json

# Find all external partners (outlinks)
jq '.edges[] | select(.type=="links_to")' target_graph.json

# Find all mentioned companies
jq '.nodes[] | select(.type=="company")' target_graph.json
```

### 4. Competitive Intelligence

```bash
# Extract all PDFs from competitor
./pipelines/extract_domain_pdfs.sh "competitor.com"

# Find all companies they mention
python3 -c "
import json
from modules.linklater.enrichment.entity_patterns import EntityExtractor

with open('competitor.com_pdfs.json') as f:
    docs = json.load(f)

extractor = EntityExtractor()
all_companies = set()

for doc in docs:
    entities = extractor.extract_all(doc['content'])
    for company in entities.get('companies', []):
        all_companies.add(company['text'])

for company in sorted(all_companies):
    print(company)
"
```

---

## üîç Common Crawl WebGraph Integration

LinkLater works with CC WebGraph for advanced link analysis:

### Download WebGraph Data

```bash
mkdir -p ~/cc_webgraph_data
cd ~/cc_webgraph_data

# Host-level graph (smaller, faster)
curl -L "https://data.commoncrawl.org/projects/hyperlinkgraph/cc-main-2025-sep-oct-nov/host/cc-main-2025-sep-oct-nov-host-edges.txt.gz" -o host-edges.txt.gz

# Domain-level graph (more detailed)
curl -L "https://data.commoncrawl.org/projects/hyperlinkgraph/cc-main-2025-sep-oct-nov/domain/cc-main-2025-sep-oct-nov-domain-edges.txt.gz" -o domain-edges.txt.gz
```

### Find Backlinks

```bash
# Find who links to your domain
zgrep "target-domain.com" host-edges.txt.gz | head -20

# Count backlinks
zgrep "target-domain.com" host-edges.txt.gz | wc -l

# Find backlinks from specific country/TLD
zgrep "target-domain.com" host-edges.txt.gz | grep "\.uk\t"
```

### Combine with LinkLater

```bash
# 1. Get backlink domains
zgrep "target.com" host-edges.txt.gz | cut -f1 | sort -u > backlink_domains.txt

# 2. Extract entities from all backlink domains
for domain in $(cat backlink_domains.txt); do
  ./pipelines/extract_domain_docs.sh "$domain" "backlinks_results/"
done

# 3. Analyze what companies link to you
jq -s 'map(.companies[]) | group_by(.text) | map({company: .[0].text, count: length})' backlinks_results/*_entities.json
```

---

## üö® Known Limitations

1. **Legacy Office formats not supported:**
   - .doc (old Word) - requires antiword CLI tool
   - .xls (old Excel) - requires xlrd library
   - .ppt (old PowerPoint) - not supported

2. **Common Crawl truncation:**
   - Files >5MB truncated in CC archives
   - Wayback/Firecrawl fallback for full files

3. **OCR not included:**
   - Scanned PDFs (images) won't extract text
   - Would need tesseract integration

4. **Backlinks require WebGraph:**
   - Not real-time (CC WebGraph updated monthly)
   - Manual download/processing required

---

## üìö Documentation

- **Full Integration Guide:** `BINARY_EXTRACTION_INTEGRATION.md`
- **Wayback Integration:** `WAYBACK_FALLBACK_ADDED.md`
- **Implementation Summary:** `IMPLEMENTATION_COMPLETE.md`
- **Test Suite:** `tests/test_binary_extraction.py`

---

## ‚úÖ Production Readiness

- ‚úÖ **Comprehensive testing**: Test suite covers all major features
- ‚úÖ **Error handling**: Graceful degradation, proper timeouts
- ‚úÖ **Performance**: LRU caching, concurrent requests
- ‚úÖ **Documentation**: Complete docs + usage examples
- ‚úÖ **Integration**: Works with Drill Search Matrix/FileType/InDOM
- ‚úÖ **Dependencies**: All required libraries installed

---

## üéâ Summary

LinkLater is a **battle-tested, production-ready** archive intelligence system that:

1. ‚úÖ Extracts text from binary files (PDF, DOCX, XLSX, PPTX)
2. ‚úÖ Three-tier fallback (CC ‚Üí Wayback ‚Üí Firecrawl)
3. ‚úÖ Extracts entities (companies, persons, registrations)
4. ‚úÖ Builds link graphs (outlinks + backlinks via CC WebGraph)
5. ‚úÖ One-command automated pipelines
6. ‚úÖ Comprehensive CLI with all flags
7. ‚úÖ Full documentation and test coverage

**No configuration needed. Just run the pipelines.**

```bash
cd /Users/attic/DRILL_SEARCH/drill-search-app/python-backend/modules/linklater/pipelines

# Extract all PDFs
./extract_domain_pdfs.sh "company.com"

# Extract all documents + entities
./extract_domain_docs.sh "company.com"

# Full knowledge graph
./full_entity_extraction.sh "company.com" company_graph.json
```

üöÄ **Ready for production use.**
